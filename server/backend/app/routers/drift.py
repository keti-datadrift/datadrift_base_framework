import os
import uuid
from typing import Optional
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException, Query
from pydantic import BaseModel
from sqlalchemy.orm import Session

from app.database import SessionLocal
from app.models import Dataset, DriftResult, AnalysisTask, EDAResult
from app.services.drift_service import run_drift
from app.services.task_queue import get_task_queue
from app.services.progress_tracker import TimeEstimator

router = APIRouter(prefix="/drift", tags=["drift"])


# DB Dependency
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# Request Body
class DriftRequest(BaseModel):
    base_id: str
    target_id: str
    force: Optional[bool] = False  # ê°•ì œ ì¬ë¶„ì„ ì˜µì…˜


# -------------------------------
# COMMON Response Wrapper
# -------------------------------
def wrap_drift_response(
    base: Dataset, 
    target: Dataset, 
    result: dict, 
    version: str,
    cached: bool = False
):
    """
    drift_service.pyì˜ ê²°ê³¼(result)ë¥¼ ì•ˆì •ì ì¸ JSON í˜•íƒœë¡œ ê°ì‹¸ì£¼ëŠ” ê³µí†µ í¬ë§·
    """

    return {
        "version": version,
        "cached": cached,
        "meta": {
            "base": {
                "id": base.id,
                "name": base.name,
                "type": base.type,
                "dvc_path": base.dvc_path,
            },
            "target": {
                "id": target.id,
                "name": target.name,
                "type": target.type,
                "dvc_path": target.dvc_path,
            },
        },
        "result": result,   # drift_service.py ì˜ í•µì‹¬ ë¶„ì„ê°’
    }


def _get_eda_cache(db: Session, dataset_id: str) -> dict:
    """
    ë°ì´í„°ì…‹ì˜ EDA ìºì‹œ(ì†ì„± ë¶„ì„, ì„ë² ë”©)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Returns:
        dict: {image_analysis: {...}, clustering: {...}} ë˜ëŠ” ë¹ˆ dict
    """
    eda_result = db.query(EDAResult).filter(EDAResult.dataset_id == dataset_id).first()
    
    if not eda_result or not eda_result.stats:
        return {}
    
    cache = {}
    
    # ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ ìºì‹œ
    if eda_result.stats.get("image_analysis"):
        cache["image_analysis"] = eda_result.stats["image_analysis"]
    
    # í´ëŸ¬ìŠ¤í„°ë§ ìºì‹œ (ì„ë² ë”© í¬í•¨)
    if eda_result.stats.get("clustering"):
        cache["clustering"] = eda_result.stats["clustering"]
    
    return cache


def _get_or_create_drift_result(
    db: Session,
    base: Dataset,
    target: Dataset,
    force: bool = False
) -> tuple[dict, bool]:
    """
    ë“œë¦¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì—ì„œ ì¡°íšŒí•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    
    EDA ë¶„ì„ ê²°ê³¼(ì†ì„±, ì„ë² ë”©)ê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©í•˜ì—¬ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    
    Returns:
        tuple: (ë¶„ì„ ê²°ê³¼ dict, ìºì‹œ ì—¬ë¶€ bool)
    """
    # 1) ë“œë¦¬í”„íŠ¸ ê²°ê³¼ ìºì‹œ ì¡°íšŒ (forceê°€ ì•„ë‹ ë•Œë§Œ)
    if not force:
        cached = db.query(DriftResult).filter(
            DriftResult.base_id == base.id,
            DriftResult.target_id == target.id
        ).first()
        
        if cached and cached.summary:
            return cached.summary, True
    
    # 2) EDA ìºì‹œ ì¡°íšŒ (ì†ì„± ë¶„ì„, ì„ë² ë”©)
    base_cache = _get_eda_cache(db, base.id)
    target_cache = _get_eda_cache(db, target.id)
    
    # ìºì‹œ ìƒíƒœ ë¡œê¹…
    print(f"ğŸ“Š ë“œë¦¬í”„íŠ¸ ë¶„ì„ - EDA ìºì‹œ ìƒíƒœ:")
    print(f"   Base ({base.name}): ì†ì„±={bool(base_cache.get('image_analysis'))}, ì„ë² ë”©={bool(base_cache.get('clustering', {}).get('embeddings'))}")
    print(f"   Target ({target.name}): ì†ì„±={bool(target_cache.get('image_analysis'))}, ì„ë² ë”©={bool(target_cache.get('clustering', {}).get('embeddings'))}")
    
    # 3) ë¶„ì„ ìˆ˜í–‰ (EDA ìºì‹œ ì „ë‹¬)
    result = run_drift(
        base.dvc_path, 
        target.dvc_path,
        base_cache=base_cache,
        target_cache=target_cache
    )
    
    # 4) ê²°ê³¼ ì €ì¥ (upsert)
    existing = db.query(DriftResult).filter(
        DriftResult.base_id == base.id,
        DriftResult.target_id == target.id
    ).first()
    
    # overall ì ìˆ˜ ì¶”ì¶œ (advanced_driftì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’)
    overall_score = None
    if "advanced_drift" in result and result["advanced_drift"]:
        ensemble = result["advanced_drift"].get("ensemble", {})
        overall_score = ensemble.get("overall_score")
    
    if existing:
        # ì—…ë°ì´íŠ¸
        existing.summary = result
        existing.feature_drift = result.get("drift") or result.get("advanced_drift")
        existing.overall = overall_score
    else:
        # ìƒˆë¡œ ìƒì„±
        drift_result = DriftResult(
            id=str(uuid.uuid4()),
            base_id=base.id,
            target_id=target.id,
            summary=result,
            feature_drift=result.get("drift") or result.get("advanced_drift"),
            overall=overall_score,
        )
        db.add(drift_result)
    
    db.commit()
    
    return result, False


# ============================================================
# Drift V1
# ============================================================

@router.post("/")
def drift(req: DriftRequest, db: Session = Depends(get_db)):
    """
    ë‘ ë°ì´í„°ì…‹ ê°„ì˜ ë“œë¦¬í”„íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    - ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìºì‹œì—ì„œ ë°˜í™˜
    - ìºì‹œê°€ ì—†ê±°ë‚˜ force=trueì´ë©´ ìƒˆë¡œ ë¶„ì„ í›„ ì €ì¥
    """
    base = db.query(Dataset).filter(Dataset.id == req.base_id).first()
    target = db.query(Dataset).filter(Dataset.id == req.target_id).first()

    if not base or not target:
        raise HTTPException(status_code=404, detail="Dataset not found")

    result, cached = _get_or_create_drift_result(db, base, target, req.force or False)

    return wrap_drift_response(base, target, result, version="v1", cached=cached)


# ============================================================
# Drift V2 (í™•ì¥ í¬ë§·)
# ============================================================

@router.post("/v2")
def drift_v2(req: DriftRequest, db: Session = Depends(get_db)):
    """
    ë‘ ë°ì´í„°ì…‹ ê°„ì˜ ë“œë¦¬í”„íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (V2 í¬ë§·)
    
    - ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìºì‹œì—ì„œ ë°˜í™˜
    - ìºì‹œê°€ ì—†ê±°ë‚˜ force=trueì´ë©´ ìƒˆë¡œ ë¶„ì„ í›„ ì €ì¥
    """
    base = db.query(Dataset).filter(Dataset.id == req.base_id).first()
    target = db.query(Dataset).filter(Dataset.id == req.target_id).first()

    if not base or not target:
        raise HTTPException(status_code=404, detail="Dataset not found")

    result, cached = _get_or_create_drift_result(db, base, target, req.force or False)

    return wrap_drift_response(base, target, result, version="v2", cached=cached)


# ============================================================
# ì‘ì—… ìƒíƒœ ì¡°íšŒ
# ============================================================

@router.get("/task/{task_id}")
def get_drift_task_status(task_id: str, db: Session = Depends(get_db)):
    """ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì‘ì—…ì˜ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    task = db.query(AnalysisTask).filter(AnalysisTask.id == task_id).first()
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    return {
        "task_id": task.id,
        "dataset_id": task.dataset_id,
        "target_id": task.target_id,
        "task_type": task.task_type,
        "status": task.status,
        "progress": task.progress,
        "message": task.message,
        "error": task.error,
        "metadata": task.task_metadata,
        "created_at": task.created_at.isoformat() if task.created_at else None,
        "started_at": task.started_at.isoformat() if task.started_at else None,
        "completed_at": task.completed_at.isoformat() if task.completed_at else None,
    }


# ============================================================
# ë¹„ë™ê¸° ë“œë¦¬í”„íŠ¸ ë¶„ì„
# ============================================================

@router.post("/async")
def drift_async(req: DriftRequest, db: Session = Depends(get_db)):
    """
    ë“œë¦¬í”„íŠ¸ ë¶„ì„ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤ (ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€).
    
    Returns:
        - status: queued | already_running | completed
        - task_id: ì‘ì—… ID (í´ë§/WebSocket ì—°ê²°ìš©)
    """
    # 1) ë°ì´í„°ì…‹ í™•ì¸
    base = db.query(Dataset).filter(Dataset.id == req.base_id).first()
    target = db.query(Dataset).filter(Dataset.id == req.target_id).first()
    
    if not base or not target:
        raise HTTPException(status_code=404, detail="Dataset not found")
    
    task_queue = get_task_queue()
    
    # 2) ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
    existing_task_id = task_queue.is_running(req.base_id, "drift", req.target_id)
    if existing_task_id:
        return {
            "status": "already_running",
            "task_id": existing_task_id,
            "message": "ì´ë¯¸ ë“œë¦¬í”„íŠ¸ ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤."
        }
    
    # 3) ìºì‹œ í™•ì¸ (forceê°€ ì•„ë‹ ë•Œë§Œ)
    if not req.force:
        cached = db.query(DriftResult).filter(
            DriftResult.base_id == req.base_id,
            DriftResult.target_id == req.target_id
        ).first()
        
        if cached and cached.summary:
            return {
                "status": "completed",
                "cached": True,
                "message": "ì´ë¯¸ ë¶„ì„ ê²°ê³¼ê°€ ì¡´ì¬í•©ë‹ˆë‹¤."
            }
    
    # 4) ìƒˆ ì‘ì—… ìƒì„±
    task_id = str(uuid.uuid4())
    
    # ì˜ˆìƒ ì‹œê°„ ê³„ì‚° (ZIP ë°ì´í„°ì…‹ì˜ ê²½ìš° ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ ê¸°ë°˜)
    estimated_time = None
    total_files = 0
    
    if base.type.lower() == "zip" and target.type.lower() == "zip":
        # ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ ì¶”ì •
        from app.services.eda_service import collect_image_files
        
        base_dir = base.dvc_path
        target_dir = target.dvc_path
        
        for f in os.listdir(base_dir):
            if f.lower().endswith(".zip"):
                extracted_dir = os.path.join(base_dir, f) + "_extracted"
                if os.path.isdir(extracted_dir):
                    total_files += len(collect_image_files(extracted_dir))
                break
        
        for f in os.listdir(target_dir):
            if f.lower().endswith(".zip"):
                extracted_dir = os.path.join(target_dir, f) + "_extracted"
                if os.path.isdir(extracted_dir):
                    total_files += len(collect_image_files(extracted_dir))
                break
        
        estimated_time = TimeEstimator.estimate_time("drift", total_files)
    
    task = AnalysisTask(
        id=task_id,
        dataset_id=req.base_id,
        target_id=req.target_id,
        task_type="drift",
        status="pending",
        progress=0.0,
        message="ëŒ€ê¸° ì¤‘...",
        task_metadata={
            "total_files": total_files,
            "estimated_seconds": estimated_time,
            "estimated_formatted": TimeEstimator.format_time(estimated_time),
        }
    )
    db.add(task)
    db.commit()
    
    # 5) ì‘ì—… íì— ë“±ë¡
    if not task_queue.start_task(task_id, req.base_id, "drift", req.target_id):
        return {
            "status": "already_running",
            "task_id": task_queue.is_running(req.base_id, "drift", req.target_id),
            "message": "ì´ë¯¸ ë“œë¦¬í”„íŠ¸ ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤."
        }
    
    # 6) ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
    task_queue.submit_task(
        _run_drift_task,
        task_id,
        req.base_id,
        req.target_id,
        base.dvc_path,
        target.dvc_path,
        req.force or False
    )
    
    return {
        "status": "queued",
        "task_id": task_id,
        "message": "ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "estimated_seconds": estimated_time,
        "estimated_formatted": TimeEstimator.format_time(estimated_time),
    }


# ============================================================
# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹¤í–‰ í•¨ìˆ˜
# ============================================================

def _run_drift_task(
    task_id: str, 
    base_id: str, 
    target_id: str,
    base_path: str,
    target_path: str,
    force: bool
):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì‹¤í–‰ (EDA ìºì‹œ í™œìš©)"""
    from app.services.progress_tracker import ProgressTracker
    
    db = SessionLocal()
    task_queue = get_task_queue()
    
    try:
        # ìƒíƒœ ì—…ë°ì´íŠ¸: ì‹œì‘
        task = db.query(AnalysisTask).filter(AnalysisTask.id == task_id).first()
        if task:
            task.status = "in_progress"
            task.started_at = datetime.utcnow()
            task.message = "ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì‹œì‘..."
            task.progress = 0.1
            db.commit()
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ í•¨ìˆ˜ (ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨)
        def update_progress(progress: float, message: str):
            nonlocal task
            try:
                task = db.query(AnalysisTask).filter(AnalysisTask.id == task_id).first()
                if task:
                    task.progress = progress
                    task.message = message
                    db.commit()
            except Exception as e:
                db.rollback()
                print(f"âš ï¸ update_progress DB ì˜¤ë¥˜: {e}")
        
        update_progress(0.15, "EDA ìºì‹œ ì¡°íšŒ ì¤‘...")
        
        # EDA ìºì‹œ ì¡°íšŒ
        base_cache = _get_eda_cache(db, base_id)
        target_cache = _get_eda_cache(db, target_id)
        
        # ìºì‹œ ìƒíƒœ ë¡œê¹…
        base_has_attrs = bool(base_cache.get('image_analysis'))
        base_has_embs = bool(base_cache.get('clustering', {}).get('embeddings'))
        target_has_attrs = bool(target_cache.get('image_analysis'))
        target_has_embs = bool(target_cache.get('clustering', {}).get('embeddings'))
        
        print(f"ğŸ“Š ë“œë¦¬í”„íŠ¸ ë¶„ì„ - EDA ìºì‹œ ìƒíƒœ:")
        print(f"   Base: ì†ì„±={base_has_attrs}, ì„ë² ë”©={base_has_embs}")
        print(f"   Target: ì†ì„±={target_has_attrs}, ì„ë² ë”©={target_has_embs}")
        
        update_progress(0.2, "ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ë¶„ì„ ìˆ˜í–‰ (EDA ìºì‹œ ì „ë‹¬)
        update_progress(0.3, "ë“œë¦¬í”„íŠ¸ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        result = run_drift(
            base_path, 
            target_path,
            base_cache=base_cache,
            target_cache=target_cache
        )
        
        update_progress(0.8, "ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ê²°ê³¼ ì €ì¥
        existing = db.query(DriftResult).filter(
            DriftResult.base_id == base_id,
            DriftResult.target_id == target_id
        ).first()
        
        overall_score = None
        if "advanced_drift" in result and result["advanced_drift"]:
            ensemble = result["advanced_drift"].get("ensemble", {})
            overall_score = ensemble.get("overall_score")
        
        if existing:
            existing.summary = result
            existing.feature_drift = result.get("drift") or result.get("advanced_drift")
            existing.overall = overall_score
        else:
            drift_result = DriftResult(
                id=str(uuid.uuid4()),
                base_id=base_id,
                target_id=target_id,
                summary=result,
                feature_drift=result.get("drift") or result.get("advanced_drift"),
                overall=overall_score,
            )
            db.add(drift_result)
        
        # ì™„ë£Œ
        task = db.query(AnalysisTask).filter(AnalysisTask.id == task_id).first()
        if task:
            task.status = "completed"
            task.completed_at = datetime.utcnow()
            task.progress = 1.0
            task.message = "ì™„ë£Œ"
        
        db.commit()
        print(f"âœ… ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì™„ë£Œ: task_id={task_id}")
        
    except Exception as e:
        task = db.query(AnalysisTask).filter(AnalysisTask.id == task_id).first()
        if task:
            task.status = "failed"
            task.completed_at = datetime.utcnow()
            task.error = str(e)
            task.message = "ì‹¤íŒ¨"
            db.commit()
        print(f"âš ï¸ ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì‹¤íŒ¨: task_id={task_id}, error={e}")
        
    finally:
        task_queue.finish_task(base_id, "drift", target_id)
        db.close()