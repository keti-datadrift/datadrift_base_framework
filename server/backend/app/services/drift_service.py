import os
import json
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from scipy import stats

from app.services.zip_resolver import analyze_zip_dataset, analyze_roboflow
from app.utils.json_sanitize import clean_json_value
from app.services.eda_service import run_image_analysis, collect_image_files
from app.services.analyzer_init import get_analyzer_service


# ë“œë¦¬í”„íŠ¸ ìƒíƒœ ì„ê³„ê°’
THRESHOLD_WARNING = 0.15
THRESHOLD_CRITICAL = 0.25


# ============================================================
# ZIP vs ZIP DRIFT
# ============================================================

def compute_zip_drift(base_info, target_info):
    """
    ZIP dataset drift:
        - split distribution ë³€í™”
        - class distribution ë³€í™”
        - annotation ìˆ˜ ë³€í™”
        - ë””ë ‰í† ë¦¬ êµ¬ì¡° ë³€í™”
        - ì´ë¯¸ì§€ ìˆ˜ ë³€í™”
    """

    zip_type = base_info.get("zip_type")

    drift = {
        "zip_type": zip_type,
        "structure": {},
        "splits": {},
        "classes": {},
        "summary": {},
    }

    # ----------------------------------------------------------
    # 1. êµ¬ì¡° ë¹„êµ
    # ----------------------------------------------------------
    drift["structure"] = {
        "base_subdirs": base_info["stats"].get("subdirs", []),
        "target_subdirs": target_info["stats"].get("subdirs", []),
    }

    # ----------------------------------------------------------
    # 2. Roboflow ZIP Drift
    # ----------------------------------------------------------
    if zip_type == "roboflow":
        b = analyze_roboflow(base_info)
        t = analyze_roboflow(target_info)

        # split drift
        split_names = ["train", "valid", "test"]
        for s in split_names:
            drift["splits"][s] = {
                "base": b["splits"][s]["num_images"],
                "target": t["splits"][s]["num_images"],
                "delta": t["splits"][s]["num_images"] - b["splits"][s]["num_images"],
            }

        # class drift
        all_classes = set(b["classes"]) | set(t["classes"])
        for c in all_classes:
            base_cnt = sum([b["splits"][s]["class_counts"].get(c, 0) for s in split_names])
            target_cnt = sum([t["splits"][s]["class_counts"].get(c, 0) for s in split_names])

            drift["classes"][c] = {
                "base": base_cnt,
                "target": target_cnt,
                "delta": target_cnt - base_cnt,
            }

        drift["summary"] = {
            "total_images_base": sum([b["splits"][s]["num_images"] for s in split_names]),
            "total_images_target": sum([t["splits"][s]["num_images"] for s in split_names]),
        }

        return drift

    # ----------------------------------------------------------
    # 3. ì¼ë°˜ YOLO / COCO / VOC Drift (ë‹¨ìˆœ í†µê³„)
    # ----------------------------------------------------------
    # ì´ë¯¸ì§€ ê°œìˆ˜ ë³€í™”
    drift["summary"]["base_images"] = base_info["stats"]["image_files"]
    drift["summary"]["target_images"] = target_info["stats"]["image_files"]
    drift["summary"]["delta_images"] = (
        target_info["stats"]["image_files"] - base_info["stats"]["image_files"]
    )

    # í…ìŠ¤íŠ¸/annotation ë³€í™”
    drift["summary"]["base_text"] = base_info["stats"]["text_files"]
    drift["summary"]["target_text"] = target_info["stats"]["text_files"]
    drift["summary"]["delta_text"] = (
        target_info["stats"]["text_files"] - base_info["stats"]["text_files"]
    )

    return drift


# ============================================================
# CSV vs CSV DRIFT
# ============================================================

def compute_csv_drift(base_path, target_path):
    print('base_path = ', base_path)
    print('target_path = ', target_path)
    df1 = pd.read_csv(base_path)
    df2 = pd.read_csv(target_path)
    print(df1)
    print(df2)

    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ë¹„êµ
    numeric_cols = [c for c in df1.columns if pd.api.types.is_numeric_dtype(df1[c])]

    drift = {}

    for col in numeric_cols:
        mean1 = df1[col].mean()
        mean2 = df2[col].mean()
        drift[col] = {
            "base_mean": None if pd.isna(mean1) else round(float(mean1), 4),
            "target_mean": None if pd.isna(mean2) else round(float(mean2), 4),
            "delta": None if pd.isna(mean1) or pd.isna(mean2) else round(float(mean2 - mean1), 4),
        }
    print('drift = ', drift)

    return drift


# ============================================================
# ZIP vs CSV / TEXT vs CSV / unsupported
# ============================================================

def run_drift(
    base_path, 
    target_path, 
    base_cache: Optional[Dict[str, Any]] = None,
    target_cache: Optional[Dict[str, Any]] = None
):
    """
    base_path, target_path:
        ZIP â†’ dataset_dir (í´ë”)
        CSV/TXT/IMAGE â†’ ë‹¨ì¼ íŒŒì¼
        
    base_cache, target_cache:
        EDA ê²°ê³¼ ìºì‹œ (image_analysis, clustering í¬í•¨)
        ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì‹œ ì¬ì‚¬ìš©í•˜ì—¬ ì†ë„ í–¥ìƒ
    """

    # -------------------
    # ZIP vs ZIP
    # -------------------
    if os.path.isdir(base_path) and os.path.isdir(target_path):
        # raw.zip ì°¾ê¸°
        def locate_raw_zip(path):
            for f in os.listdir(path):
                if f.lower().endswith(".zip"):
                    return os.path.join(path, f)
            return None

        base_zip = locate_raw_zip(base_path)
        target_zip = locate_raw_zip(target_path)

        if not base_zip or not target_zip:
            raise RuntimeError("ZIP dataset drift: raw.zip íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        base_info = analyze_zip_dataset(base_zip)
        target_info = analyze_zip_dataset(target_zip)

        result = compute_zip_drift(base_info, target_info)
        result["type"] = "zip_zip"
        
        # ì´ë¯¸ì§€ ì†ì„±/ì„ë² ë”© ê¸°ë°˜ ê³ ê¸‰ ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì¶”ê°€ (ìºì‹œ ì‚¬ìš©)
        try:
            advanced_drift = compute_advanced_image_drift(
                base_info.get("root_dir"),
                target_info.get("root_dir"),
                base_cache=base_cache,
                target_cache=target_cache
            )
            if advanced_drift:
                result["advanced_drift"] = advanced_drift
        except Exception as e:
            print(f"âš ï¸ ê³ ê¸‰ ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return clean_json_value(result)

    # -------------------
    # CSV vs CSV
    # -------------------
    if base_path.endswith(".csv") and target_path.endswith(".csv"):
        result = compute_csv_drift(base_path, target_path)
        return clean_json_value({"type": "csv_csv", "drift": result})

    # -------------------
    # ë‹¤ë¥¸ ì¡°í•©ì€ ë¯¸ì§€ì› (í–¥í›„ ì¶”ê°€ ê°€ëŠ¥)
    # -------------------
    return clean_json_value({
        "type": "unsupported",
        "message": f"Drift not supported between {base_path} and {target_path}"
    })


# ============================================================
# ê³ ê¸‰ ì´ë¯¸ì§€ ë“œë¦¬í”„íŠ¸ ë¶„ì„ (ì•™ìƒë¸” ë©”íŠ¸ë¦­)
# ============================================================

def compute_advanced_image_drift(
    base_dir: str, 
    target_dir: str,
    base_cache: Optional[Dict[str, Any]] = None,
    target_cache: Optional[Dict[str, Any]] = None
) -> Optional[Dict[str, Any]]:
    """
    ë‘ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ê°„ì˜ ê³ ê¸‰ ë“œë¦¬í”„íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    EDA ìºì‹œê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©í•˜ì—¬ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    
    ë¶„ì„ ë©”íŠ¸ë¦­:
    - ì†ì„± ë“œë¦¬í”„íŠ¸: KL Divergence (í¬ê¸°, ë…¸ì´ì¦ˆ, ì„ ëª…ë„)
    - ì„ë² ë”© ë“œë¦¬í”„íŠ¸: MMD, Mean Shift, Wasserstein, PSI
    - ì•™ìƒë¸” ì ìˆ˜ ë° ìƒíƒœ íŒì •
    
    Args:
        base_dir: ê¸°ì¤€ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬
        target_dir: ë¹„êµ ëŒ€ìƒ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬
        base_cache: ê¸°ì¤€ ë°ì´í„°ì…‹ì˜ EDA ìºì‹œ (image_analysis, clustering)
        target_cache: ë¹„êµ ë°ì´í„°ì…‹ì˜ EDA ìºì‹œ
        
    Returns:
        dict: ë“œë¦¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼
    """
    if not base_dir or not target_dir:
        return None
    
    if not os.path.isdir(base_dir) or not os.path.isdir(target_dir):
        return None
    
    print(f"ğŸ” ê³ ê¸‰ ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì‹œì‘")
    print(f"   Base: {base_dir}")
    print(f"   Target: {target_dir}")
    
    # ìºì‹œ ì‚¬ìš© ì—¬ë¶€ ë¡œê¹…
    base_attrs_cached = base_cache and base_cache.get("image_analysis")
    target_attrs_cached = target_cache and target_cache.get("image_analysis")
    base_embs_cached = base_cache and base_cache.get("clustering") and base_cache["clustering"].get("embeddings")
    target_embs_cached = target_cache and target_cache.get("clustering") and target_cache["clustering"].get("embeddings")
    
    if base_attrs_cached:
        print("   âœ… Base ì†ì„± ë°ì´í„°: ìºì‹œ ì‚¬ìš©")
    if target_attrs_cached:
        print("   âœ… Target ì†ì„± ë°ì´í„°: ìºì‹œ ì‚¬ìš©")
    if base_embs_cached:
        print("   âœ… Base ì„ë² ë”© ë°ì´í„°: ìºì‹œ ì‚¬ìš©")
    if target_embs_cached:
        print("   âœ… Target ì„ë² ë”© ë°ì´í„°: ìºì‹œ ì‚¬ìš©")
    
    # ============================================================
    # 1. ì†ì„± ë°ì´í„° ì¤€ë¹„ (ìºì‹œ ë˜ëŠ” ìƒˆë¡œ ë¶„ì„)
    # ============================================================
    base_attrs = {}
    target_attrs = {}
    
    if base_attrs_cached:
        # ìºì‹œì—ì„œ ì†ì„± ë°ì´í„° ë¡œë“œ
        base_attrs = base_cache["image_analysis"].get("attributes", {})
    else:
        # ìƒˆë¡œ ë¶„ì„
        print("   ğŸ“Š Base ì†ì„± ë¶„ì„ ì¤‘...")
        base_images = collect_image_files(base_dir)
        analyzer = get_analyzer_service()
        for img in base_images:
            rel_path = os.path.relpath(img, base_dir)
            attrs = analyzer.analyze_image_attributes(img)
            if attrs:
                base_attrs[rel_path] = attrs
    
    if target_attrs_cached:
        # ìºì‹œì—ì„œ ì†ì„± ë°ì´í„° ë¡œë“œ
        target_attrs = target_cache["image_analysis"].get("attributes", {})
    else:
        # ìƒˆë¡œ ë¶„ì„
        print("   ğŸ“Š Target ì†ì„± ë¶„ì„ ì¤‘...")
        target_images = collect_image_files(target_dir)
        analyzer = get_analyzer_service()
        for img in target_images:
            rel_path = os.path.relpath(img, target_dir)
            attrs = analyzer.analyze_image_attributes(img)
            if attrs:
                target_attrs[rel_path] = attrs
    
    if not base_attrs or not target_attrs:
        print("   âš ï¸ ì†ì„± ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # 2. ì†ì„± ë“œë¦¬í”„íŠ¸ (KL Divergence)
    attribute_drift = compute_attribute_drift(base_attrs, target_attrs)
    
    # ============================================================
    # 3. ì„ë² ë”© ë°ì´í„° ì¤€ë¹„ (ìºì‹œ ë˜ëŠ” ìƒˆë¡œ ì¶”ì¶œ)
    # ============================================================
    embedding_drift = None
    
    if len(base_attrs) >= 5 and len(target_attrs) >= 5:
        base_embs = []
        target_embs = []
        
        # Base ì„ë² ë”©
        if base_embs_cached:
            base_embs = base_cache["clustering"]["embeddings"]
            print(f"   ğŸ“¦ Base ì„ë² ë”© ìºì‹œ ë¡œë“œ: {len(base_embs)}ê°œ")
        else:
            print("   ğŸ”¬ Base ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
            base_images = collect_image_files(base_dir)
            analyzer = get_analyzer_service()
            for img in base_images:
                emb = analyzer.extract_embedding(img)
                if emb and 'embedding' in emb:
                    base_embs.append(emb['embedding'])
        
        # Target ì„ë² ë”©
        if target_embs_cached:
            target_embs = target_cache["clustering"]["embeddings"]
            print(f"   ğŸ“¦ Target ì„ë² ë”© ìºì‹œ ë¡œë“œ: {len(target_embs)}ê°œ")
        else:
            print("   ğŸ”¬ Target ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
            target_images = collect_image_files(target_dir)
            analyzer = get_analyzer_service()
            for img in target_images:
                emb = analyzer.extract_embedding(img)
                if emb and 'embedding' in emb:
                    target_embs.append(emb['embedding'])
        
        if len(base_embs) >= 5 and len(target_embs) >= 5:
            embedding_drift = compute_embedding_drift(
                np.array(base_embs), 
                np.array(target_embs)
            )
    
    # 4. ì•™ìƒë¸” ì ìˆ˜ ê³„ì‚°
    ensemble_result = compute_ensemble_drift_score(attribute_drift, embedding_drift)
    
    result = {
        "file_counts": {
            "base": len(base_attrs),
            "target": len(target_attrs),
        },
        "attribute_drift": attribute_drift,
        "embedding_drift": embedding_drift,
        "ensemble": ensemble_result,
        "cache_used": {
            "base_attributes": bool(base_attrs_cached),
            "target_attributes": bool(target_attrs_cached),
            "base_embeddings": bool(base_embs_cached),
            "target_embeddings": bool(target_embs_cached),
        }
    }
    
    print(f"âœ… ê³ ê¸‰ ë“œë¦¬í”„íŠ¸ ë¶„ì„ ì™„ë£Œ")
    print(f"   ì „ì²´ ì ìˆ˜: {ensemble_result['overall_score']:.4f}")
    print(f"   ìƒíƒœ: {ensemble_result['status']}")
    
    return result


def compute_attribute_drift(base_attrs: Dict, target_attrs: Dict) -> Dict[str, Any]:
    """
    ì†ì„± ë¶„í¬ ê°„ ë“œë¦¬í”„íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    ë©”íŠ¸ë¦­: KL Divergence
    """
    # ì†ì„±ê°’ ì¶”ì¶œ
    base_sizes = [v['size'] for v in base_attrs.values() if 'size' in v]
    target_sizes = [v['size'] for v in target_attrs.values() if 'size' in v]
    
    base_noise = [v['noise_level'] for v in base_attrs.values() if 'noise_level' in v]
    target_noise = [v['noise_level'] for v in target_attrs.values() if 'noise_level' in v]
    
    base_sharp = [v['sharpness'] for v in base_attrs.values() if 'sharpness' in v]
    target_sharp = [v['sharpness'] for v in target_attrs.values() if 'sharpness' in v]
    
    drift = {}
    
    # KL Divergence ê³„ì‚°
    if base_sizes and target_sizes:
        drift['size'] = {
            'kl_divergence': calculate_kl_divergence(base_sizes, target_sizes),
            'base_mean': round(float(np.mean(base_sizes)), 4),
            'target_mean': round(float(np.mean(target_sizes)), 4),
            'base_std': round(float(np.std(base_sizes)), 4),
            'target_std': round(float(np.std(target_sizes)), 4),
        }
    
    if base_noise and target_noise:
        drift['noise'] = {
            'kl_divergence': calculate_kl_divergence(base_noise, target_noise),
            'base_mean': round(float(np.mean(base_noise)), 4),
            'target_mean': round(float(np.mean(target_noise)), 4),
            'base_std': round(float(np.std(base_noise)), 4),
            'target_std': round(float(np.std(target_noise)), 4),
        }
    
    if base_sharp and target_sharp:
        drift['sharpness'] = {
            'kl_divergence': calculate_kl_divergence(base_sharp, target_sharp),
            'base_mean': round(float(np.mean(base_sharp)), 4),
            'target_mean': round(float(np.mean(target_sharp)), 4),
            'base_std': round(float(np.std(base_sharp)), 4),
            'target_std': round(float(np.std(target_sharp)), 4),
        }
    
    # íˆìŠ¤í† ê·¸ë¨ ë°ì´í„° (ì‹œê°í™”ìš©)
    drift['distributions'] = {
        'size': create_comparison_histogram(base_sizes, target_sizes, 20),
        'noise': create_comparison_histogram(base_noise, target_noise, 20),
        'sharpness': create_comparison_histogram(base_sharp, target_sharp, 20),
    }
    
    return drift


def compute_embedding_drift(base_embs: np.ndarray, target_embs: np.ndarray) -> Dict[str, Any]:
    """
    ì„ë² ë”© ê³µê°„ì—ì„œì˜ ë“œë¦¬í”„íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    ë©”íŠ¸ë¦­:
    - MMD (Maximum Mean Discrepancy)
    - Mean Shift
    - Wasserstein Distance
    - PSI (Population Stability Index)
    """
    drift = {}
    
    # 1. MMD
    drift['mmd'] = calculate_mmd(base_embs, target_embs)
    
    # 2. Mean Shift
    base_mean = base_embs.mean(axis=0)
    target_mean = target_embs.mean(axis=0)
    mean_shift = np.linalg.norm(base_mean - target_mean)
    drift['mean_shift'] = round(float(mean_shift / np.sqrt(base_embs.shape[1])), 4)
    
    # 3. Wasserstein Distance (1D projection)
    try:
        proj_base = base_embs.mean(axis=1)
        proj_target = target_embs.mean(axis=1)
        drift['wasserstein'] = round(float(stats.wasserstein_distance(proj_base, proj_target)), 4)
    except Exception:
        drift['wasserstein'] = 0.0
    
    # 4. PSI on PCA components
    try:
        from sklearn.decomposition import PCA
        n_components = min(5, base_embs.shape[1], base_embs.shape[0], target_embs.shape[0])
        pca = PCA(n_components=n_components)
        base_pca = pca.fit_transform(base_embs)
        target_pca = pca.transform(target_embs)
        
        psi_scores = []
        for dim in range(n_components):
            psi = calculate_psi(base_pca[:, dim], target_pca[:, dim])
            psi_scores.append(psi)
        
        drift['psi'] = round(float(np.mean(psi_scores)), 4)
        drift['psi_max'] = round(float(np.max(psi_scores)), 4)
    except Exception:
        drift['psi'] = 0.0
        drift['psi_max'] = 0.0
    
    # 5. Cosine Distance
    base_mean_norm = np.linalg.norm(base_mean)
    target_mean_norm = np.linalg.norm(target_mean)
    if base_mean_norm > 0 and target_mean_norm > 0:
        cos_sim = np.dot(base_mean, target_mean) / (base_mean_norm * target_mean_norm)
        drift['cosine_distance'] = round(float(max(0, 1 - cos_sim)), 4)
    else:
        drift['cosine_distance'] = 0.0
    
    return drift


def compute_ensemble_drift_score(
    attribute_drift: Optional[Dict], 
    embedding_drift: Optional[Dict]
) -> Dict[str, Any]:
    """
    ì†ì„± ë° ì„ë² ë”© ë“œë¦¬í”„íŠ¸ë¥¼ ì¢…í•©í•œ ì•™ìƒë¸” ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    scores = {}
    weights = {}
    
    # ì†ì„± ë“œë¦¬í”„íŠ¸ ì ìˆ˜
    if attribute_drift:
        for attr in ['size', 'noise', 'sharpness']:
            if attr in attribute_drift:
                kl = attribute_drift[attr].get('kl_divergence', 0)
                # KLì„ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                scores[f'attr_{attr}'] = min(kl / 0.5, 1.0)
                weights[f'attr_{attr}'] = 0.1
    
    # ì„ë² ë”© ë“œë¦¬í”„íŠ¸ ì ìˆ˜
    if embedding_drift:
        # MMD
        if 'mmd' in embedding_drift:
            scores['emb_mmd'] = min(embedding_drift['mmd'] / 0.5, 1.0)
            weights['emb_mmd'] = 0.25
        
        # Mean Shift
        if 'mean_shift' in embedding_drift:
            scores['emb_mean_shift'] = min(embedding_drift['mean_shift'] / 0.1, 1.0)
            weights['emb_mean_shift'] = 0.2
        
        # Wasserstein
        if 'wasserstein' in embedding_drift:
            scores['emb_wasserstein'] = min(embedding_drift['wasserstein'] / 1.0, 1.0)
            weights['emb_wasserstein'] = 0.15
        
        # PSI
        if 'psi' in embedding_drift:
            scores['emb_psi'] = min(embedding_drift['psi'] / 0.25, 1.0)
            weights['emb_psi'] = 0.1
    
    # ê°€ì¤‘ í‰ê· 
    if scores and weights:
        total_weight = sum(weights.values())
        overall_score = sum(scores[k] * weights[k] for k in scores) / total_weight
    else:
        overall_score = 0.0
    
    # ìƒíƒœ íŒì •
    if overall_score < THRESHOLD_WARNING:
        status = "NORMAL"
    elif overall_score < THRESHOLD_CRITICAL:
        status = "WARNING"
    else:
        status = "CRITICAL"
    
    return {
        "overall_score": round(overall_score, 4),
        "status": status,
        "component_scores": {k: round(v, 4) for k, v in scores.items()},
        "weights": weights,
        "thresholds": {
            "warning": THRESHOLD_WARNING,
            "critical": THRESHOLD_CRITICAL,
        }
    }


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================

def calculate_kl_divergence(p: List[float], q: List[float], bins: int = 20) -> float:
    """KL Divergenceë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        # ê³µí†µ ë²”ìœ„ ê²°ì •
        min_val = min(min(p), min(q))
        max_val = max(max(p), max(q))
        
        # íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°
        p_hist, edges = np.histogram(p, bins=bins, range=(min_val, max_val), density=True)
        q_hist, _ = np.histogram(q, bins=edges, density=True)
        
        # 0 ë°©ì§€
        p_hist = p_hist + 1e-10
        q_hist = q_hist + 1e-10
        
        # ì •ê·œí™”
        p_hist = p_hist / p_hist.sum()
        q_hist = q_hist / q_hist.sum()
        
        # KL Divergence
        kl = float(np.sum(p_hist * np.log(p_hist / q_hist)))
        return round(abs(kl), 4)
    except Exception:
        return 0.0


def calculate_mmd(X: np.ndarray, Y: np.ndarray, gamma: float = 1.0) -> float:
    """Maximum Mean Discrepancyë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        # ìƒ˜í”Œ í¬ê¸° ì œí•œ
        if X.shape[0] > 500:
            X = X[:500]
        if Y.shape[0] > 500:
            Y = Y[:500]
        
        # ì •ê·œí™”
        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)
        Y = (Y - Y.mean(axis=0)) / (Y.std(axis=0) + 1e-8)
        
        XX = np.dot(X, X.T)
        YY = np.dot(Y, Y.T)
        XY = np.dot(X, Y.T)
        
        X_sqnorms = np.diagonal(XX)
        Y_sqnorms = np.diagonal(YY)
        
        def rbf_kernel(X_sqnorms, Y_sqnorms, XY):
            K = -2 * XY + X_sqnorms[:, None] + Y_sqnorms[None, :]
            K = np.clip(K, -50, 50)
            return np.exp(-gamma * K)
        
        K_XX = rbf_kernel(X_sqnorms, X_sqnorms, XX)
        K_YY = rbf_kernel(Y_sqnorms, Y_sqnorms, YY)
        K_XY = rbf_kernel(X_sqnorms, Y_sqnorms, XY)
        
        m = X.shape[0]
        n = Y.shape[0]
        
        mmd = np.sum(np.triu(K_XX, k=1)) / (m * (m - 1) / 2)
        mmd += np.sum(np.triu(K_YY, k=1)) / (n * (n - 1) / 2)
        mmd -= 2 * K_XY.sum() / (m * n)
        
        return round(float(np.sqrt(max(mmd, 0))), 4)
    except Exception:
        return 0.0


def calculate_psi(baseline: np.ndarray, current: np.ndarray, bins: int = 10) -> float:
    """Population Stability Indexë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        min_val = min(baseline.min(), current.min())
        max_val = max(baseline.max(), current.max())
        edges = np.linspace(min_val, max_val, bins + 1)
        
        baseline_hist, _ = np.histogram(baseline, bins=edges)
        current_hist, _ = np.histogram(current, bins=edges)
        
        baseline_prop = (baseline_hist + 1) / (baseline_hist.sum() + bins)
        current_prop = (current_hist + 1) / (current_hist.sum() + bins)
        
        psi = np.sum((current_prop - baseline_prop) * np.log(current_prop / baseline_prop))
        return round(float(abs(psi)), 4)
    except Exception:
        return 0.0


def create_comparison_histogram(
    base_data: List[float], 
    target_data: List[float], 
    bins: int = 20
) -> Dict[str, Any]:
    """ë¹„êµìš© íˆìŠ¤í† ê·¸ë¨ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if not base_data or not target_data:
        return {}
    
    try:
        min_val = min(min(base_data), min(target_data))
        max_val = max(max(base_data), max(target_data))
        
        base_hist, edges = np.histogram(base_data, bins=bins, range=(min_val, max_val))
        target_hist, _ = np.histogram(target_data, bins=edges)
        
        bin_centers = [(edges[i] + edges[i+1]) / 2 for i in range(len(base_hist))]
        
        return {
            "bins": [round(b, 4) for b in bin_centers],
            "base": base_hist.tolist(),
            "target": target_hist.tolist(),
        }
    except Exception:
        return {}