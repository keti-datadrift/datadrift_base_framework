import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Optional, List, Dict, Any

from app.utils.json_sanitize import clean_json_value
from app.services.zip_resolver import (
    analyze_zip_dataset,
    analyze_roboflow,
    analyze_yolo,
    analyze_voc,
    analyze_coco,
)
from app.services.analyzer_init import get_analyzer_service

# ì´ë¯¸ì§€ í™•ì¥ì
IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp', '.tiff'}


def run_eda(file_path: str, dtype: str = "csv") -> dict:
    """
    file_path:
        CSV â†’ raw.csv
        ZIP â†’ dvc_storage/datasets/<id>/   (í´ë”)
        TEXT â†’ raw.txt
        IMAGE â†’ raw.png

    dtype: csv | zip | text | image | unknown
    """
    dtype = dtype.lower()

    # =========================================================
    # ZIP DATASET EDA
    # =========================================================
    if dtype == "zip":
        # file_path = dvc_storage/datasets/<id>/
        dataset_dir = file_path

        # raw.zip ì°¾ê¸°
        raw_zip = None
        for f in os.listdir(dataset_dir):
            if f.lower().endswith(".zip"):
                raw_zip = os.path.join(dataset_dir, f)
                break

        if not raw_zip:
            raise RuntimeError(f"ZIP Dataset EDA: raw.zip íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {dataset_dir}")

        # ZIP ë¶„ì„ (roboflow/yolo/voc/coco ìë™ ê°ì§€)
        info = analyze_zip_dataset(raw_zip)

        zip_type = info.get("zip_type", "unknown")

        result = {
            "type": "zip",
            "zip_type": zip_type,
            "tree": info.get("tree"),
            "stats": info.get("stats"),
            "images": info.get("images", []),
        }

        # -----------------------
        # Roboflow EDA
        # -----------------------
        if zip_type == "roboflow":
            result["roboflow"] = analyze_roboflow(info)

        # -----------------------
        # YOLO EDA
        # -----------------------
        elif zip_type == "yolo":
            result["yolo"] = analyze_yolo(info)

        # -----------------------
        # VOC EDA
        # -----------------------
        elif zip_type == "voc":
            result["voc"] = analyze_voc(info)

        # -----------------------
        # COCO EDA
        # -----------------------
        elif zip_type == "coco":
            result["coco"] = analyze_coco(info)

        # ì‹¬ì¸µ ë¶„ì„ (ì´ë¯¸ì§€ ì†ì„±, ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§)ì€ ë³„ë„ APIë¡œ ë¶„ë¦¬
        # /eda/{id}/image-analysis ë° /eda/{id}/clustering ì°¸ì¡°
        result["root_dir"] = info.get("root_dir")  # ì‹¬ì¸µ ë¶„ì„ìš© ê²½ë¡œ ì €ì¥

        return clean_json_value(result)

    # =========================================================
    # CSV EDA
    # =========================================================
    if dtype == "csv":
        df = pd.read_csv(file_path)

        # inf â†’ NaN
        df = df.replace([np.inf, -np.inf], np.nan)

        # NaN â†’ "" (Pandas v2ì—ì„œ None ë¶ˆê°€)
        df = df.fillna("")

        # Preview (5 rows)
        preview = df.head(5).to_dict(orient="records")

        # Summary
        summary = df.describe(include="all").replace({np.nan: None}).to_dict()

        # Missing rate
        missing_rate = (
            df.replace("", np.nan).isna().mean().round(4).replace({np.nan: None}).to_dict()
        )

        return clean_json_value({
            "type": "csv",
            "shape": df.shape,
            "missing_rate": missing_rate,
            "summary": summary,
            "preview": preview,
        })

    # =========================================================
    # TEXT / JSON EDA
    # =========================================================
    if dtype == "text":
        with open(file_path, "r", errors="ignore") as f:
            lines = f.readlines()

        # JSON íŒŒì¼ì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ìš°ì„  íŒŒì‹± ì‹œë„
        json_data = None
        try:
            with open(file_path, "r", errors="ignore") as j:
                json_data = json.load(j)
        except Exception:
            pass  # TEXTë¡œ ì²˜ë¦¬

        result = {
            "type": "text",
            "num_lines": len(lines),
            "first_lines": lines[:20],
        }

        if json_data:
            result["json"] = json_data

        return clean_json_value(result)

    # =========================================================
    # IMAGE EDA
    # =========================================================
    if dtype == "image":
        # ë‹¨ì¼ ì´ë¯¸ì§€ì´ë¯€ë¡œ ê°„ë‹¨í•˜ê²Œ metadataë§Œ ë°˜í™˜
        return {
            "type": "image",
            "image_path": file_path,
            "info": "Single image dataset (no annotations)",
        }

    # =========================================================
    # UNKNOWN TYPE
    # =========================================================
    return clean_json_value({
        "type": dtype,
        "info": f"EDA not implemented for type {dtype}",
    })


def run_image_attributes(directory: str) -> Optional[Dict[str, Any]]:
    """
    ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤ (ë¹ ë¦„).
    
    - ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ (í¬ê¸°, ë…¸ì´ì¦ˆ, ì„ ëª…ë„, í’ˆì§ˆ ì ìˆ˜)
    - ë¶„í¬ ë°ì´í„° ê³„ì‚°
    
    Args:
        directory: ë¶„ì„í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        dict: ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ ê²°ê³¼
    """
    try:
        image_files = collect_image_files(directory)
        
        if not image_files:
            return None
        
        print(f"ğŸ“Š ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ ì‹œì‘: {len(image_files)}ê°œ íŒŒì¼")
        
        analyzer = get_analyzer_service()
        
        # ì´ë¯¸ì§€ ì†ì„± ë¶„ì„
        attr_results = {}
        for img_path in image_files:
            rel_path = os.path.relpath(img_path, directory)
            attrs = analyzer.analyze_image_attributes(img_path)
            if attrs:
                attr_results[rel_path] = attrs
        
        if not attr_results:
            return None
        
        # ìš”ì•½ í†µê³„ ê³„ì‚°
        summary_stats = calculate_summary_stats(attr_results)
        
        result = {
            "num_images": len(attr_results),
            "summary": summary_stats,
            "attributes": attr_results,
            "distributions": calculate_distributions(attr_results),
        }
        
        print(f"âœ… ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ ì™„ë£Œ: {len(attr_results)}ê°œ íŒŒì¼")
        return result
        
    except Exception as e:
        print(f"âš ï¸ ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None


def run_image_attributes_with_progress(
    directory: str,
    progress_callback=None,
    tracker=None
) -> Optional[Dict[str, Any]]:
    """
    ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ (ì§„í–‰ë¥  ì½œë°± ì§€ì› ë²„ì „).
    
    Args:
        directory: ë¶„ì„í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (progress, message)
        tracker: ProgressTracker ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        dict: ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ ê²°ê³¼
    """
    try:
        image_files = collect_image_files(directory)
        
        if not image_files:
            return None
        
        total_files = len(image_files)
        print(f"ğŸ“Š ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ ì‹œì‘: {total_files}ê°œ íŒŒì¼")
        
        if progress_callback:
            progress_callback(0.05, f"ì´ë¯¸ì§€ {total_files}ê°œ ë¶„ì„ ì‹œì‘...")
        
        analyzer = get_analyzer_service()
        
        # ì´ë¯¸ì§€ ì†ì„± ë¶„ì„
        attr_results = {}
        update_interval = max(1, total_files // 20)  # 5% ê°„ê²©ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        
        for idx, img_path in enumerate(image_files):
            rel_path = os.path.relpath(img_path, directory)
            attrs = analyzer.analyze_image_attributes(img_path)
            if attrs:
                attr_results[rel_path] = attrs
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if tracker:
                tracker.update(1)
            
            if progress_callback and (idx % update_interval == 0 or idx == total_files - 1):
                progress = 0.1 + (idx + 1) / total_files * 0.8  # 10% ~ 90%
                progress_callback(progress, f"ì´ë¯¸ì§€ ë¶„ì„ ì¤‘... ({idx + 1}/{total_files})")
        
        if not attr_results:
            return None
        
        if progress_callback:
            progress_callback(0.92, "í†µê³„ ê³„ì‚° ì¤‘...")
        
        # ìš”ì•½ í†µê³„ ê³„ì‚°
        summary_stats = calculate_summary_stats(attr_results)
        
        if progress_callback:
            progress_callback(0.96, "ë¶„í¬ ë°ì´í„° ìƒì„± ì¤‘...")
        
        result = {
            "num_images": len(attr_results),
            "summary": summary_stats,
            "attributes": attr_results,
            "distributions": calculate_distributions(attr_results),
        }
        
        if progress_callback:
            progress_callback(1.0, "ì™„ë£Œ")
        
        print(f"âœ… ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ ì™„ë£Œ: {len(attr_results)}ê°œ íŒŒì¼")
        return result
        
    except Exception as e:
        print(f"âš ï¸ ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None


def run_image_clustering(directory: str, save_embeddings: bool = True) -> Optional[Dict[str, Any]]:
    """
    ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ ë° í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤ (ëŠë¦¼).
    
    - CLIP ì„ë² ë”© ì¶”ì¶œ
    - K-means í´ëŸ¬ìŠ¤í„°ë§
    - 2D ì‹œê°í™”ìš© ì¢Œí‘œ ê³„ì‚°
    
    Args:
        directory: ë¶„ì„í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        save_embeddings: ì›ë³¸ ì„ë² ë”© ë²¡í„° ì €ì¥ ì—¬ë¶€ (ë“œë¦¬í”„íŠ¸ ë¶„ì„ìš©)
        
    Returns:
        dict: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (save_embeddings=Trueë©´ ì›ë³¸ ì„ë² ë”© í¬í•¨)
    """

    try:
        image_files = collect_image_files(directory)
        
        if not image_files or len(image_files) < 5:
            return {"error": "í´ëŸ¬ìŠ¤í„°ë§ì—ëŠ” ìµœì†Œ 5ê°œ ì´ìƒì˜ ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤."}
        
        print(f"ğŸ”¬ ì´ë¯¸ì§€ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {len(image_files)}ê°œ íŒŒì¼")
        
        analyzer = get_analyzer_service()
        
        embeddings_list = []
        file_names = []
        file_paths = []
        
        for img_path in image_files:
            rel_path = os.path.relpath(img_path, directory)
            emb = analyzer.extract_embedding(img_path)
            if emb and 'embedding' in emb:
                embeddings_list.append(emb['embedding'])
                file_names.append(rel_path)
                file_paths.append(img_path)
        
        if len(embeddings_list) < 5:
            return {"error": "ì„ë² ë”© ì¶”ì¶œ ê°€ëŠ¥í•œ ì´ë¯¸ì§€ê°€ 5ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤."}
        
        clustering_result = analyzer.perform_clustering(
            embeddings_list, file_names, file_paths,
            n_clusters=None, method='kmeans'
        )
        
        if not clustering_result or 'clustering_results' not in clustering_result:
            return {"error": "í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨"}
        
        embeddings_2d = clustering_result['clustering_results'].get('embeddings_2d')
        
        result = {
            "num_images": len(embeddings_list),
            "n_clusters": clustering_result.get('n_clusters'),
            "cluster_stats": clustering_result.get('cluster_stats'),
            "embeddings_2d": embeddings_2d,
            "cluster_labels": clustering_result.get('clustering_results', {}).get('cluster_labels'),
            "file_names": file_names,
        }
        
        # ì›ë³¸ ì„ë² ë”© ì €ì¥ (ë“œë¦¬í”„íŠ¸ ë¶„ì„ì—ì„œ ì¬ì‚¬ìš©)
        if save_embeddings:
            # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy arrayë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ (JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ)
            result["embeddings"] = [emb.tolist() if hasattr(emb, 'tolist') else emb for emb in embeddings_list]
        
        print(f"âœ… ì´ë¯¸ì§€ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(embeddings_list)}ê°œ íŒŒì¼, {result['n_clusters']}ê°œ í´ëŸ¬ìŠ¤í„°")
        return result
        
    except Exception as e:
        print(f"âš ï¸ ì´ë¯¸ì§€ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None


def run_image_clustering_with_progress(
    directory: str,
    progress_callback=None,
    tracker=None,
    save_embeddings: bool = True
) -> Optional[Dict[str, Any]]:
    """
    ì´ë¯¸ì§€ í´ëŸ¬ìŠ¤í„°ë§ (ì§„í–‰ë¥  ì½œë°± ì§€ì› ë²„ì „).
    
    Args:
        directory: ë¶„ì„í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (progress, message)
        tracker: ProgressTracker ì¸ìŠ¤í„´ìŠ¤
        save_embeddings: ì›ë³¸ ì„ë² ë”© ë²¡í„° ì €ì¥ ì—¬ë¶€ (ë“œë¦¬í”„íŠ¸ ë¶„ì„ìš©)
        
    Returns:
        dict: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (save_embeddings=Trueë©´ ì›ë³¸ ì„ë² ë”© í¬í•¨)
    """
    try:
        image_files = collect_image_files(directory)
        
        if not image_files or len(image_files) < 5:
            return {"error": "í´ëŸ¬ìŠ¤í„°ë§ì—ëŠ” ìµœì†Œ 5ê°œ ì´ìƒì˜ ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤."}
        
        total_files = len(image_files)
        print(f"ğŸ”¬ ì´ë¯¸ì§€ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {total_files}ê°œ íŒŒì¼")
        
        if progress_callback:
            progress_callback(0.05, f"ì„ë² ë”© ì¶”ì¶œ ì‹œì‘... ({total_files}ê°œ ì´ë¯¸ì§€)")
        
        analyzer = get_analyzer_service()
        
        embeddings_list = []
        file_names = []
        file_paths = []
        update_interval = max(1, total_files // 20)  # 5% ê°„ê²©ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        
        for idx, img_path in enumerate(image_files):
            rel_path = os.path.relpath(img_path, directory)
            emb = analyzer.extract_embedding(img_path)
            if emb and 'embedding' in emb:
                embeddings_list.append(emb['embedding'])
                file_names.append(rel_path)
                file_paths.append(img_path)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if tracker:
                tracker.update(1)
            
            if progress_callback and (idx % update_interval == 0 or idx == total_files - 1):
                progress = 0.1 + (idx + 1) / total_files * 0.7  # 10% ~ 80%
                progress_callback(progress, f"ì„ë² ë”© ì¶”ì¶œ ì¤‘... ({idx + 1}/{total_files})")
        
        if len(embeddings_list) < 5:
            return {"error": "ì„ë² ë”© ì¶”ì¶œ ê°€ëŠ¥í•œ ì´ë¯¸ì§€ê°€ 5ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤."}
        
        if progress_callback:
            progress_callback(0.85, "K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘...")
        
        clustering_result = analyzer.perform_clustering(
            embeddings_list, file_names, file_paths,
            n_clusters=None, method='kmeans'
        )
        
        if not clustering_result or 'clustering_results' not in clustering_result:
            return {"error": "í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨"}
        
        if progress_callback:
            progress_callback(0.95, "ê²°ê³¼ ì •ë¦¬ ì¤‘...")
        
        embeddings_2d = clustering_result['clustering_results'].get('embeddings_2d')
        
        result = {
            "num_images": len(embeddings_list),
            "n_clusters": clustering_result.get('n_clusters'),
            "cluster_stats": clustering_result.get('cluster_stats'),
            "embeddings_2d": embeddings_2d,
            "cluster_labels": clustering_result.get('clustering_results', {}).get('cluster_labels'),
            "file_names": file_names,
        }
        
        # ì›ë³¸ ì„ë² ë”© ì €ì¥ (ë“œë¦¬í”„íŠ¸ ë¶„ì„ì—ì„œ ì¬ì‚¬ìš©)
        if save_embeddings:
            result["embeddings"] = [emb.tolist() if hasattr(emb, 'tolist') else emb for emb in embeddings_list]
        
        if progress_callback:
            progress_callback(1.0, "ì™„ë£Œ")
        
        print(f"âœ… ì´ë¯¸ì§€ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(embeddings_list)}ê°œ íŒŒì¼, {result['n_clusters']}ê°œ í´ëŸ¬ìŠ¤í„°")
        return result
        
    except Exception as e:
        print(f"âš ï¸ ì´ë¯¸ì§€ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None


def run_image_analysis(directory: str) -> Optional[Dict[str, Any]]:
    """
    ZIP ì••ì¶• í•´ì œëœ ë””ë ‰í† ë¦¬ì—ì„œ ì „ì²´ ì´ë¯¸ì§€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤ (ë ˆê±°ì‹œ í˜¸í™˜).
    
    - ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ (í¬ê¸°, ë…¸ì´ì¦ˆ, ì„ ëª…ë„, í’ˆì§ˆ ì ìˆ˜)
    - CLIP ì„ë² ë”© ì¶”ì¶œ
    - K-means í´ëŸ¬ìŠ¤í„°ë§
    
    Args:
        directory: ë¶„ì„í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        dict: ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼
    """
    try:
        # ì†ì„± ë¶„ì„
        attr_result = run_image_attributes(directory)
        if not attr_result:
            return None
        
        result = attr_result.copy()
        
        # í´ëŸ¬ìŠ¤í„°ë§ (5ê°œ ì´ìƒì¼ ë•Œë§Œ)
        if attr_result.get("num_images", 0) >= 5:
            clustering_result = run_image_clustering(directory)
            if clustering_result and "error" not in clustering_result:
                result["clustering"] = {
                    "n_clusters": clustering_result.get('n_clusters'),
                    "cluster_stats": clustering_result.get('cluster_stats'),
                    "embeddings_2d": clustering_result.get('embeddings_2d'),
                    "cluster_labels": clustering_result.get('cluster_labels'),
                }
        
        return result
        
    except Exception as e:
        print(f"âš ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None


def collect_image_files(directory: str) -> List[str]:
    """
    ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Args:
        directory: ê²€ìƒ‰í•  ë””ë ‰í† ë¦¬
        
    Returns:
        list: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    image_files = []
    
    for root, _, files in os.walk(directory):
        for f in files:
            ext = os.path.splitext(f)[1].lower()
            if ext in IMAGE_EXTENSIONS:
                image_files.append(os.path.join(root, f))
    
    return image_files


def calculate_summary_stats(attr_results: Dict[str, dict]) -> Dict[str, Any]:
    """
    ì†ì„± ë¶„ì„ ê²°ê³¼ì˜ ìš”ì•½ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        attr_results: íŒŒì¼ë³„ ì†ì„± ë¶„ì„ ê²°ê³¼
        
    Returns:
        dict: ìš”ì•½ í†µê³„
    """
    if not attr_results:
        return {}
    
    sizes = [v['size'] for v in attr_results.values() if 'size' in v]
    noise_levels = [v['noise_level'] for v in attr_results.values() if 'noise_level' in v]
    sharpness_values = [v['sharpness'] for v in attr_results.values() if 'sharpness' in v]
    widths = [v['width'] for v in attr_results.values() if 'width' in v]
    heights = [v['height'] for v in attr_results.values() if 'height' in v]
    
    # í˜•ì‹ë³„ í†µê³„
    formats = {}
    resolutions = {}
    for item in attr_results.values():
        fmt = item.get('format', 'unknown')
        formats[fmt] = formats.get(fmt, 0) + 1
        
        res = item.get('resolution', 'unknown')
        resolutions[res] = resolutions.get(res, 0) + 1
    
    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    quality_scores = []
    if noise_levels and sharpness_values:
        for noise, sharp in zip(noise_levels, sharpness_values):
            # í’ˆì§ˆ ì ìˆ˜: ì„ ëª…ë„ ë†’ì„ìˆ˜ë¡, ë…¸ì´ì¦ˆ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            sharp_norm = min(sharp / 100, 1.0)
            noise_norm = max(0, 1.0 - (noise / 0.5))  # ë…¸ì´ì¦ˆ 0.5 ì´ìƒì´ë©´ 0ì 
            quality = (sharp_norm * 0.6 + noise_norm * 0.4) * 100
            quality_scores.append(quality)
    
    return {
        "total_images": len(attr_results),
        "total_size_mb": round(sum(sizes), 2) if sizes else 0,
        "avg_size_mb": round(np.mean(sizes), 4) if sizes else 0,
        "avg_width": round(np.mean(widths), 1) if widths else 0,
        "avg_height": round(np.mean(heights), 1) if heights else 0,
        "formats": formats,
        "resolutions": dict(sorted(resolutions.items(), key=lambda x: x[1], reverse=True)[:10]),
        "size_stats": {
            "min": round(float(np.min(sizes)), 4) if sizes else 0,
            "max": round(float(np.max(sizes)), 4) if sizes else 0,
            "mean": round(float(np.mean(sizes)), 4) if sizes else 0,
            "std": round(float(np.std(sizes)), 4) if sizes else 0,
        } if sizes else {},
        "noise_stats": {
            "min": round(float(np.min(noise_levels)), 4) if noise_levels else 0,
            "max": round(float(np.max(noise_levels)), 4) if noise_levels else 0,
            "mean": round(float(np.mean(noise_levels)), 4) if noise_levels else 0,
            "std": round(float(np.std(noise_levels)), 4) if noise_levels else 0,
        } if noise_levels else {},
        "sharpness_stats": {
            "min": round(float(np.min(sharpness_values)), 4) if sharpness_values else 0,
            "max": round(float(np.max(sharpness_values)), 4) if sharpness_values else 0,
            "mean": round(float(np.mean(sharpness_values)), 4) if sharpness_values else 0,
            "std": round(float(np.std(sharpness_values)), 4) if sharpness_values else 0,
        } if sharpness_values else {},
        "quality_stats": {
            "min": round(float(np.min(quality_scores)), 2) if quality_scores else 0,
            "max": round(float(np.max(quality_scores)), 2) if quality_scores else 0,
            "mean": round(float(np.mean(quality_scores)), 2) if quality_scores else 0,
            "std": round(float(np.std(quality_scores)), 2) if quality_scores else 0,
        } if quality_scores else {},
    }


def calculate_distributions(attr_results: Dict[str, dict]) -> Dict[str, Any]:
    """
    ì°¨íŠ¸ìš© ë¶„í¬ ë°ì´í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        attr_results: íŒŒì¼ë³„ ì†ì„± ë¶„ì„ ê²°ê³¼
        
    Returns:
        dict: ë¶„í¬ ë°ì´í„° (íˆìŠ¤í† ê·¸ë¨, ì‚°ì ë„ìš©)
    """
    if not attr_results:
        return {}
    
    sizes = [v['size'] for v in attr_results.values() if 'size' in v]
    noise_levels = [v['noise_level'] for v in attr_results.values() if 'noise_level' in v]
    sharpness_values = [v['sharpness'] for v in attr_results.values() if 'sharpness' in v]
    
    distributions = {}
    
    # íŒŒì¼ í¬ê¸° ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨ìš©)
    if sizes:
        hist, bin_edges = np.histogram(sizes, bins=20)
        distributions["size"] = {
            "bins": [round((bin_edges[i] + bin_edges[i+1]) / 2, 4) for i in range(len(hist))],
            "counts": hist.tolist(),
        }
    
    # ë…¸ì´ì¦ˆ ë¶„í¬
    if noise_levels:
        hist, bin_edges = np.histogram(noise_levels, bins=20)
        distributions["noise"] = {
            "bins": [round((bin_edges[i] + bin_edges[i+1]) / 2, 4) for i in range(len(hist))],
            "counts": hist.tolist(),
        }
    
    # ì„ ëª…ë„ ë¶„í¬
    if sharpness_values:
        hist, bin_edges = np.histogram(sharpness_values, bins=20)
        distributions["sharpness"] = {
            "bins": [round((bin_edges[i] + bin_edges[i+1]) / 2, 4) for i in range(len(hist))],
            "counts": hist.tolist(),
        }
    
    # í’ˆì§ˆ ë§µ (ë…¸ì´ì¦ˆ vs ì„ ëª…ë„ ì‚°ì ë„ìš©)
    if noise_levels and sharpness_values and len(noise_levels) == len(sharpness_values):
        # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ 100ê°œë¡œ ì œí•œ)
        indices = list(range(len(noise_levels)))
        if len(indices) > 100:
            import random
            indices = random.sample(indices, 100)
        
        distributions["quality_map"] = {
            "noise": [round(noise_levels[i], 4) for i in indices],
            "sharpness": [round(sharpness_values[i], 4) for i in indices],
        }
    
    return distributions